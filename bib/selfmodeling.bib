@inproceedings{10.1145/3537972.3537991,
author = {Tsuchida, Shuhei and Mao, Haomin and Okamoto, Hideaki and Suzuki, Yuma and Kanada, Rintaro and Hori, Takayuki and Terada, Tsutomu and Tsukamoto, Masahiko},
title = {Dance Practice System That Shows What You Would Look Like If You Could Master the Dance},
year = {2022},
isbn = {9781450387163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537972.3537991},
doi = {10.1145/3537972.3537991},
abstract = {This study proposes a dance practice system allowing users to learn dancing by watching videos in which they have mastered the movements of a professional dancer. Video self-modeling, which encourages learners to improve their behavior by watching videos of exemplary behavior by themselves, effectively teaches movement skills. However, creating an ideal dance movement video is time-consuming and tedious for learners. To solve this problem, we utilize a video generation technique based on deepfake to automatically generate a video of the learners dancing the same movement as the dancer in the reference video. We conducted a user study with 20 participants to verify whether the deepfake video effectively teaches dance movements. The results showed no significant difference between the groups learning with the original and deepfake videos. In addition, the group using the deepfake video had significantly lower self-efficacy. Based on these experimental results, we discussed the design implications of the system using the deepfake video to support learning dance movements.},
booktitle = {Proceedings of the 8th International Conference on Movement and Computing},
articleno = {15},
numpages = {8},
keywords = {learning, deepfake, video self-modeling, dance movements, skill acquisition},
location = {Chicago, IL, USA},
series = {MOCO '22}
}